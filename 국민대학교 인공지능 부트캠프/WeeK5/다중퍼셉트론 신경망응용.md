- 이번에 한 내용은 다중 퍼셉트론신경망을 이용해서 패션아이템을 구분하는 실습이었다.

#### 필요한 라이브러리를 `import`한다.
```python
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import plot_model,to_categorical
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
```

#### `keras`에서 제공하는 `fashion_mnist`데이터셋을 불러온다
``` python
(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()
```

#### 데이터셋에 있는 이미지들을 확인해 본다.
```python
fig,axes=plt.subplots(nrows=10,ncols=10,figsize=(13,25),
                      subplot_kw={"xticks":[],"yticks":[]})

labels=["T-shirt/top",
        "Trouser",
        "Pullover",
        "Dress",
        "Coat",
        "Sandal",
        "Shirt",
        "Sneaker",
        "Bag",
        "Ankle boot"]


for i,ax in enumerate(axes.flat):
    ax.imshow(x_train[i],cmap="gray")
    title=labels[y_train[i]]
    ax.set_title(title)
plt.show()
```

#### 모델학습에 용이하도록 아래와 같이 데이터 전처리를 한다.
  - 3차원을 2차원으로
  - 정수를 실수로 변경
  - 0과255사이의 수치를 0.0과1.0사이로 정규화 수행
  - 0과9사이에 있는 라벨에 대한 원핫코딩 수행
  
```python
fig,axes=plt.subplots(nrows=10,ncols=10,figsize=(13,25),
                      subplot_kw={"xticks":[],"yticks":[]})

labels=["T-shirt/top",
        "Trouser",
        "Pullover",
        "Dress",
        "Coat",
        "Sandal",
        "Shirt",
        "Sneaker",
        "Bag",
        "Ankle boot"]


for i,ax in enumerate(axes.flat):
    ax.imshow(x_train[i],cmap="gray")
    title=labels[y_train[i]]
    ax.set_title(title)
plt.show()
```
#### 모델을 구성하고 ,컴파일 한다.
모델은 다음과 같이 구성하였습니다.  
  - 입력: 1차원의 784개 벡터
  - 출력: 10개 벡터
  - 출력측의 활성화 함수:소프트맥스
  - 손실함수:categorical_crossentropy
  - 최적화기: adam
  - 평가 메트릭:accuracy
```python
model = Sequential([
    Dense(2000,input_dim=28*28,activation="relu"),
    Dense(128,activation="relu"),
    Dense(128,activation="relu"),
    Dense(64,activation="relu"),
    Dense(10,activation="softmax")
])
model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
```

#### 체크포인트를 설정해서 오버피팅이 일어나지 않는 순간을 캐치한다.
``` python
checkpoint_callback=ModelCheckpoint("best_model.h5",
                                    save_best_only=True,
                                    moniter="val_loss")

hist=model.fit(x_train,y_train,validation_split=0.2,
               batch_size=32,epochs=100,callbacks=[checkpoint_callback])
```

#### 검증셋을 이용해서 모델을 평가한다.
```python
model=load_model("best_model.h5")

test_loss,test_acc=model.evaluate(x_test,y_test)

print("Test accuracy ",test_acc)
print("Test loss ",test_loss)
```
- 실행결과
`313/313 [==============================] - 1s 4ms/step - loss: 0.3414 - accuracy: 0.8813
Test accuracy  0.8812999725341797
Test loss  0.341426283121109`

- 추가로 모델의 성능을 높이고 싶다면 Dense추가하거나 바꾸면된다. 아니면 네트워크 아키텍쳐를 바꿔본느 것도 좋다.

느낀 점: 계속해서 다중퍼셈트론을 이용하고 있어서 어렵지는 않았다. 하지만 이번에 다룬 내용은 내가 생각하는 인공지능 모델의
응용과 가장 가까이 있어서 재미있었다~

