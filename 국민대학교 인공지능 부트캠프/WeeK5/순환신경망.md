- 순환신경망은 고정된 데이터에 대한 것이 아니라 시계열 데이터와 같이 계속해서 변하는 데이터를 흐름에 따라 예측하는 데 스이는 인공지능 모델이다,

- 이번에는 문장을 입력해서 이진분류하는 모델에 대해서 공부하였다!
- 데이터 셋은 영화리뷰에 관한 데이터이다. 일단 다층퍼셉트론 신경망모델을 먼저 이용해 보겠다

#### 필요한 라이브러리들을 임포트 한다.
```python
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.layers import Dense,Embedding,LSTM,Flatten
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import plot_model
from tensorflow.keras.datasets import imdb
```
#### 데이테셋을 받아온다.
```python
(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=20000)#20000번째로 많이 사용하는 단어까지만 데이터셋을 만듬
```

#### 영화리뷰 댓글은 문자의 길이가 다 다르다. 그렇기 때문에 모델에 넣어주기 위해선 고정된 길이로 만드는 과정을 진행한다.
- 케라스에서 제공하는 전처리 함수인 sequence의 pad_sequences()함수를 이용한다.
```python
from keras.preprocessing import sequence

x_train=sequence.pad_sequences(x_train,maxlen=200)
x_test=sequence.pad_sequences(x_test,maxlen=200)
```
#### 다층퍼셉트론 신경망 모델 Embedding레이어로 단어들을 인코딩 한 후 Dense레이어를 통해 분류하였다.
```python
model=Sequential()
model.add(Embedding(20000,128,input_length=200))
model.add(Flatten())
model.add(Dense(256,activation="relu"))
model.add(Dense(1,activation="sigmoid"))
```
- `Emedding`의 첫번째인자는 단어 사전의 크기를 말하며 총 20000개의 단어종류가 있다는 의미이다.
- 두번째인자는 단어를 인코딩 한 후 나오는 벡터크기로서 이 값이 128이란 소리는 128차원의 의미론적 기하공간에 나타낸다는 의미이다.
- 세번째인자는 단어의 수 즉 문장의 길이를 나타낸다.

#### 모델을 컴파일 한다.
```python
model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])

checkpoint_callback=ModelCheckpoint("best_model.h5",save_best_only=True,
                                    monitor="val_loss")

hist=model.fit(x_train,y_train,epochs=5,batch_size=64,
               validation_split=0.2,
               callbacks=[checkpoint_callback])
```

## 여기까지가 다층 퍼셉트론신경망을 이용한 것이다. 이제 순환신경망으로 다시 한번 모델을 설정해 보겠다.

#### 모델을 정의한다.
``` python
model=Sequential()
model.add(Embedding(20000,128))
model.add(LSTM(128))
model.add(Dense(1,activation="sigmoid"))
```
- 순환(LSTM)레이어와 Dense레이어로 층을 쌓았다.
- 이때 임베딩 레이어에 input_lengh인자를 따로 설정하지 않은 이뉴는 입력문의 길이에 따라 input_length가 자동으로 정해지고,
- 이것이 LSTM레이어에는 timesteps로 입력되기 떄문이다.

#### 모델을 컴파일 한다.
```python
model.compile(loss="binary_crossentropy",optimizer="adam",metrics=["accuracy"])

checkpoint_callback=ModelCheckpoint("best_model.h5",
                                    save_best_only=True,
                                    moniter="val_loss")

hist=model.fit(x_train,y_train,epochs=5,batch_size=64,
               validation_split=0.2,
               callbacks=[checkpoint_callback])
```

#### 모델을 평가한다.
```python
model=load_model("best_model.h5")
loss_and_metrics=model.evaluate(x_test,y_test,batch_size=64)
print("## evaluation loss_and_metrics ##")
print(loss_and_metrics)
```

느낀 점 :  순환신경망 바로 전 쳅터인 `컨볼루션 신경망`부터 먼가 잘 안된다.. fit하는 시간이 엄청나게 길어지기더 하고, 훈련셋이 갑자기 누락되기도 한다.분명 강의에 있는 코드와 똑같은데 말이다..





