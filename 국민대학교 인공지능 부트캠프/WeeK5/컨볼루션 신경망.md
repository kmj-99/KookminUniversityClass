
- CNN(Convolutional Neural Network)로도 불리며 주로 시각적 이미지를 분석하는 데 많이 사용하는 딥러닝 모델이다.  
- CNN은 이미지 전체를 작은 단위로 쪼개어서 분석하는 것이 핵심이다.

#### 필요한 라이브러리들을 import한다.
```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import urllib

from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten
from tensorflow.keras.utils import plot_model,to_categorical
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.callbacks import ModelCheckpoint
```

#### 패션이미지 데이터들을 load한다.
```python
(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()
```
#### load한 패션이미지를 보기위새 `plt.subplots()`를 한다.
```python
fig,axes=plt.subplots(nrows=10,ncols=10,figsize=(13,25),
                      subplot_kw={"xticks":[],"yticks":[]})

labels=["T-shirt/top",
        "Trouser",
        "Pullover",
        "Dress",
        "Coat",
        "Sandal",
        "Shirt",
        "Sneaker",
        "Bag",
        "Ankle boot"]


for i,ax in enumerate(axes.flat):
    ax.imshow(x_train[i],cmap="gray")
    title=labels[y_train[i]]
    ax.set_title(title)
plt.show()
```

#### 데이터 전처리를 실행한다. 이때 3차원을 4차원으로 변경한다. 다중퍼셉트론 신경망에서 했던 전처리와 똑같다.
```python
x_train=x_train.reshape(60000,28,28,1).astype("float32")/255.0 # 60000을 784의 형태로 변경
x_test=x_test.reshape(10000,28,28,1).astype("float32")/255.0 
y_train=to_categorical(y_train)
y_test=to_categorical(y_test)
```
#### 다음과 같이 모델을 구성한다. 그리고 컴파일을 진행한다.
- 입력: 1차원의 784개 벡터
- 출력: 10개 벡터
- 출력층의 활성화 함수: softmax
- 손실함수: categorical_crossentropy
- 최적화기: adam
- 평가 메트릭: accuracy
```python
model=Sequential([
                 
      Conv2D(32,(3,3),padding="same",input_shape=(28,28,1),activation="relu"),
      Conv2D(32,(3,3),padding="same",activation="relu"),
      MaxPooling2D(pool_size=(2,2)),  #이미지를 2배로 축소
      Conv2D(64,(3,3),padding="same",activation="relu"),
      Conv2D(64,(3,3),padding="same",activation="relu"),
      MaxPooling2D(pool_size=(2,2)),
      Flatten(),
      Dense(512,activation="relu"),
      Dense(10,activation="softmax"),
])

model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
```

#### 오버피팅을 최소화 하기위해서 `ModelCheckPoint`를 설정한다. 
```python
checkpoint_callback=ModelCheckpoint("best_model.h5",
                                    save_best_only=True,
                                    monitor="val_loss")

hist=model.fit(x_train,y_train,
               validation_split=0.2,
               batch_size=32,
               epochs=100,
               callbacks=[checkpoint_callback])


```
- 여담이지만 이때 시간이 장난아니게 많이걸렸다 ;;

#### 모델을 검증셋으로 `evaluate`한다.
```python
model=load_model("best_model.h5")

test_loss,test_acc=model.evaluate(x_test,y_test)
print("Test accuracy",test_acc)
print("Test loss",test_loss)
```
- 실행결과
`313/313 [==============================] - 6s 19ms/step - loss: 0.2416 - accuracy: 0.9178
Test accuracy 0.9178000092506409
Test loss 0.24159486591815948`
- 이정도면 쏘쏘하다~

- 이번 실습을 하면서..
- fit를 진행할 때 역대급으로 시간이 많이걸렸다. 한 3시간..? 정도 걸렸던 거 같다. 그래서 혹여냐 내가 코드를 잘못작성했나 하고  
- 강의에 나와있는 코드와 비교를 수없이 헸지만 잘못작성한 부분은 없었다. 대체 난 왜 이렇게 많이 걸렸지...?
- 내 노트북이 안좋은건가? 아님 구글컴퓨터가..?




























